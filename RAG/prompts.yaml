model_identity_QA_generation: |
  You are an assistant developed to generate synthetic question-answer pairs for training Retrieval-Augmented Generation (RAG) systems. 
  Your outputs will be used as supervised data for dense retrieval and QA evaluation. 
  You must strictly follow the format, produce factoid-style questions answerable from the provided context, and avoid adding information not supported by the context.


QA_generation_prompt: |
  Your task is to generate a factoid-style question and its answer given a context passage from Hugging Face documentation. 

  Guidelines:
  - The factoid question should be concise, specific, and answerable by a short span of text from the context. 
  - The question should resemble what a user might type into a search engine (e.g., "What is X?", "Which metric is used for Y?", "Who developed Z?").
  - Do NOT reference the context explicitly (e.g., avoid "according to the passage" or "in the text").
  - Do NOT create vague, opinion-based, or unanswerable questions.
  - If the context does not contain specific, factual information that can be turned into a clear factoid question with a definitive answer, output "None" for both fields.

  Format your output exactly as follows:
  Output:::
  Factoid question::: <your factoid question>
  Answer::: <the factual answer>

  ---

  ### Good Example 1

  Context:::
  Popular ML tasks like Machine Translation and Named Entity Recognition have specific metrics that can be used to compare models. For example, a series of different metrics have been proposed for text generation, ranging from BLEU and its derivatives such as GoogleBLEU and GLEU, but also ROUGE, MAUVE, etc.

  Output:::
  Factoid question::: What metric is commonly used to evaluate machine translation models?
  Answer::: BLEU

  ---

  ### Good Example 2

  Context:::
  - Name: regnety_002
    In Collection: RegNetY
    Metadata:
      FLOPs: 255754236
      Parameters: 3160000
      Training Data:
      - ImageNet
    Results:
    - Task: Image Classification
      Dataset: ImageNet
      Metrics:
        Top 1 Accuracy: 70.28%
        Top 5 Accuracy: 89.55%

  Output:::
  Factoid question::: What Top-1 accuracy does regnety_002 achieve on ImageNet?
  Answer::: 70.28%

  ---

  ### Bad Example

  Context:::
  Some datasets have specific metrics associated with them -- this is especially in the case of popular benchmarks like GLUE and SQuAD.

  Output:::
  Factoid question::: Why are GLUE and SQuAD the best benchmarks for everyone to use?
  Answer::: Because they are very popular and widely used.

  ---

  ### Example - No Valid Factoid

  Context:::
  © 2023 Hugging Face. All rights reserved. Terms of Service | Privacy Policy
  
  Output:::
  Factoid question::: None
  Answer::: None
  ---

  Now here is the context.
  Context::: {context}
  Output:::

model_identity_critique: |
  You are a Hugging Face documentation quality evaluator.
  Your purpose is to review synthetic question and answer generated from Hugging Face documentation for Retrieval-Augmented Generation (RAG) training.
  You judge the *quality* of the questions and/or answers, not generate new ones.
  Base your judgments solely on the provided question and/or answer and/or context, using clear reasoning and consistent standards.
  Be objective and concise.
  
question_groundedness_critique_prompt: |
  You will be given a context and a question.
  Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously using the given context.

  Rate from 1 to 5:
  - 1 → The question cannot be answered from the context at all.
  - 5 → The question is clearly and unambiguously answerable using the context.

  Format your output exactly as follows:

  Output:::
  Evaluation::: (your reasoning)
  Total rating::: (a number between 1 and 5)

  You MUST provide both Evaluation::: and Total rating::: fields.

  ### Example 1:
  Question::: What is the stock price of Hugging Face in 2025?
  Context::: The Transformers library provides thousands of pretrained models for text, vision, and speech tasks.
  Output:::
  Evaluation::: The context describes machine learning libraries, not financial data. The question is completely unrelated and unanswerable.
  Total rating::: 1

  ### Example 2:
  Question::: What library provides pretrained models for text, vision, and speech tasks?
  Context::: The Transformers library provides thousands of pretrained models for text, vision, and speech tasks.
  Output:::
  Evaluation::: The question directly matches the information stated in the context and can be answered exactly with the phrase “Transformers library.”
  Total rating::: 5

  Now here are the question and context.

  Question::: {question}
  Context::: {context}
  Output:::

question_relevance_critique_prompt: |
  You will be given a question.
  Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building applications with the Hugging Face ecosystem.
  Give your total rating on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.

  Format your output exactly as follows:

  Output:::
  Evaluation::: (your rationale for the rating, as a text)
  Total rating::: (your rating, as a number between 1 and 5)

  You MUST provide values for 'Evaluation:::' and 'Total rating:::' in your output.

  ### Example 1:
  Question::: What is the population of Paris?
  Output:::
  Evaluation::: This question has nothing to do with Hugging Face or machine learning. It's irrelevant to the ecosystem.
  Total rating::: 1

  ### Example 2:
  Question::: How can I fine-tune a pretrained BERT model using the Trainer API?
  Output:::
  Evaluation::: This question is directly relevant to Hugging Face developers and covers a common and valuable use case.
  Total rating::: 5

  Now here is the question.

  Question::: {question}\n
  Output:::

question_standalone_critique_prompt: |
  You will be given a question.
  Your task is to rate how independent this question is—whether it makes sense without any external context.

  Rate from 1 to 5:
  - 1 → The question depends on missing context or phrases like “in the document”.
  - 5 → The question is clear and self-contained.

  Format your output exactly as follows:

  Output:::
  Evaluation::: (your rationale for the rating, as a text)
  Total rating::: (your rating, as a number between 1 and 5)

  You MUST provide values for 'Evaluation:::' and 'Total rating:::' in your output.

  ### Example 1:
  Question::: What metric is used in the example above?
  Output:::
  Evaluation::: The question refers to “the example above,” making it impossible to understand without that specific context.
  Total rating::: 1

  ### Example 2:
  Question::: What metric is commonly used to evaluate machine translation models?
  Output:::
  Evaluation::: The question is clear, self-contained, and meaningful without referring to an external passage. 
  Total rating::: 5

  Now here is the question.

  Question::: {question}\n
  Output:::
  
model_identity_duplication: |
  You are a Hugging Face documentation duplication evaluator.
  Your purpose is to compare two documentation snippets and judge how redundant they are for Retrieval-Augmented Generation (RAG) training.
  You do NOT generate new content; you only evaluate redundancy.

  Key principle:
  - Identify whether Document B adds *meaningful new information* beyond Document A.

  Rules:
  - Ignore small differences in examples that do NOT change the meaning (e.g., different model IDs, filenames, placeholder values).
  - Documents are duplicates if the only differences are arbitrary example values.
  - BUT documents are NOT duplicates if the different example values introduce new factual information or different behavior (e.g., different accuracy numbers, different benchmark results, different API behaviors, different hyperparameter effects).

  Examples of “meaning-changing differences” (NOT duplicates):
  - Different accuracy, runtime, memory usage, or benchmarking results.
  - Different dataset statistics.
  - Different outcomes or interpretations.
  - Different model behaviors or constraints.

  The goal is to detect *true semantic duplication*, not surface similarity.

document_duplication_critique_prompt: |
  You will be given two documentation snippets (Document A and Document B).
  Your task is to evaluate how redundant Document B is with respect to Document A.
  Base your judgment strictly on the provided texts.

  Duplication rating (1–5):
  - 1 → Completely different.
  - 2 → Mostly different; small overlap.
  - 3 → Partial overlap; both contain distinct, meaningful information.
  - 4 → Highly redundant; only minor or example-level differences.
  - 5 → Near-duplicate; same substantive content, trivial variations.

  Critical rule about examples:
  - If the only difference is an arbitrary example value (e.g., model ID, file name, placeholder argument), rate as 4 or 5.
  - BUT if the example difference changes the underlying meaning (e.g., accuracy 0.95 vs 0.96, runtime differences, different results), rate lower (typically 1–3), because meaningful factual content differs.

  Output format (required):
  Output:::
  Evaluation::: (explain similarities and whether differences change meaning)
  Duplication rating::: (1–5)
  Decision::: (yes or no)

  ### Example 1: Example-only difference → duplicates
  Document A:::
  Replace the model name with the variant you want to use, e.g., `seresnext26d_32x4d`.

  Document B:::
  Replace the model name with the variant you want to use, e.g., `seresnext26d_64x4d`.

  Output:::
  Evaluation::: Only the model ID in the example differs. This is a superficial example change that does not affect meaning.
  Duplication rating::: 5
  Decision::: yes

  ### Example 2: Example changes factual meaning → NOT duplicates
  Document A:::
  The model seresnext26d_32x4d achieves an accuracy of 0.95 on ImageNet.

  Document B:::
  The model seresnext26d_64x4d achieves an accuracy of 0.96 on ImageNet.

  Output:::
  Evaluation::: Although the structure and wording are almost identical, the documents report different accuracy numbers for different model variants. These are meaningful factual differences relevant to users.
  Duplication rating::: 2
  Decision::: no

  ### Example 3: Completely unrelated → not duplicates
  Document A:::
  The Diffusers library provides tools for image generation.

  Document B:::
  Token classification assigns labels to each token in a sequence.

  Output:::
  Evaluation::: These documents discuss different libraries and tasks.
  Duplication rating::: 1
  Decision::: no

  ### Example 4: Format-only difference → duplicates
  Document A:::
  >>> # Print top categories per image
  >>> top5_prob, top5_catid = torch.topk(probabilities, 5)

  Document B:::
  # Print top categories per image

  top5_prob, top5_catid = torch.topk(probabilities, 5)

  Output:::
  Evaluation::: Both documents contain exactly the same instructions and code. The only differences are formatting. No new information is introduced.
  Duplication rating::: 5
  Decision::: yes
  
  Now here are the documents to compare.

  Document A::: {doc_a}
  Document B::: {doc_b}
  Output:::

query_retrieve_prompt_base: |
  Given a web search query, retrieve relevant passages that answer the query

query_retrieve_prompt_ML: |
  Given a machine-learning–related search query, retrieve relevant passages that answer the query

query_retrieve_prompt_ML_HF: |
  Given a machine-learning–related search query, retrieve the relevant passages from the Hugging Face documentation that answer the query
